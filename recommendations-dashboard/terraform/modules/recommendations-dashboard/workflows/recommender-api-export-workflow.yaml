# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# TODO: Scope down from these large permissions.
# Required Permissions:
# Bigquery Admin  on the BigQuery Project
# serviceusage.serviceUsageAdmin (To enable services)
# resourcemanager.projects.update on Project
# Recommenders Exporter at the org level
# Cloud Asset Viewer at the org level

main:
  # Required Arguments:
  #   orgId = The organization you want to export.
  #
  # Optional Arguments:
  #   datasetId = the dataset in bigquery you want the export to take place to.
  #   assetTable = the table in bigquery you want to export data to.
  #   levelToExport = The Org/Project/Folder level you want exports to. I.E organizations/123 or projects/my-project-id or folders/123
  #       Sadly this one won't be available soon as it doesn't look like the recommender API does exports at project level unless you call it directly.
  #   bqLocation = location of BQ you want to use.
  #   orgId = The organization required for export.
  #   projectId = project for bigquery dataset.
  params: [args]
  steps:
    - checkRequiredInputs:
        switch:
          - condition: ${not("orgId" in args)}
            raise: "Arg OrgId not specified."
          - condition: ${"orgId" in args}
            assign:
              - orgId: ${args.orgId}
    - setDefaultValues:
        assign:
          - datasetId: "recommendations_export_dataset"
          - assetTable: "asset_export_table"
          - bqLocation: "US"
          - levelToExport: ${"organizations/"+orgId}
          - projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
    - checkOptionalInputs:
        switch:
          - condition: ${"datasetId" in args}
            assign:
              - datasetId: ${args.datasetId}
          - condition: ${"assetTable" in args}
            assign:
              - assetTable: ${args.assetTable}
          - condition: ${"bqLocation" in args}
            assign:
              - bqLocation: ${args.bqLocation}
          - condition: ${"projectId" in args}
            assign:
              - projectId: ${args.projectId}
          - condition: ${"levelToExport" in args}
            assign:
              - levelToExport: ${args.levelToExport}
    - enableBigQueryApiIfNeeded:
        call: checkAndEnableApiService
        args:
          projectId: ${projectId}
          API: "bigquerydatatransfer.googleapis.com"
    - enableCloudAssetApiIfNeeded:
        call: checkAndEnableApiService
        args:
          projectId: ${projectId}
          API: "cloudasset.googleapis.com"
    - createDatasetIfNeeded:
        call: create_dataset_if_needed
        args:
          projectId: ${projectId}
          datasetId: ${datasetId}
          region: ${bqLocation}
    - enrollDataSource:
        call: enrollDataSourceIfNeeded
        args:
          projectId: ${projectId}
    - createDataExport:
        call: createDataExportIfNeeded
        args:
          projectId: ${projectId}
          orgId: ${orgId}
          datasetId: ${datasetId}
        result: configId
    - runExports:
        call: runExportsInParallel
        args:
          bqDataset: ${datasetId}
          bqTable: ${assetTable}
          projectId: ${projectId}
          orgId: ${orgId}
          levelToExport: ${levelToExport}
          transferId: ${configId}
    - returnOutput:
        return: "Finished"

# A Function to check if an API is enabled, and if not enable it.
checkAndEnableApiService:
  params: [projectId, API]
  steps:
  - checkIfServiceEnabled:
        call: http.get
        args:
          url: ${"https://serviceusage.googleapis.com/v1/projects/"+ projectId + "/services/" + API}
          auth:
            type: OAuth2
            scopes:
              - "https://www.googleapis.com/auth/cloud-platform.read-only"
        result: apiResponse
  # The problem with enabling the API is it can take some time to propagate.
  - checkApiResponse:
      switch:
      #enable the API if not enabled
        - condition: ${apiResponse.body.state != "ENABLED"}
          steps:
            - enableAPI:
                call: http.post
                args:
                  url: ${"https://serviceusage.googleapis.com/v1/projects/"+ projectId + "/services/" + API +":enable"}
                  body:
                    # body is empty according to:
                    # https://cloud.google.com/service-usage/docs/reference/rest/v1/services/enable
                  auth:
                    type: OAuth2
                    scopes:
                      - "https://www.googleapis.com/auth/cloud-platform"
                      - "https://www.googleapis.com/auth/service.management"
            - waitForAPIPropagation:
                call: sys.sleep
                args:
                  seconds: 60


# TODO (Ghaun): Updating naming convention instead of having multiple styles
# Need to lookup best practices for yaml
create_dataset_if_needed:
    params: [projectId, datasetId, region]
    steps:
    - checkIfDatasetExists:
        try:
            call: googleapis.bigquery.v2.datasets.get
            args:
                datasetId: ${datasetId}
                projectId: ${projectId}
            result: getResult
        except:
            as: e
            steps:
                - known_errors:
                    switch:
                    # Create the dataset if it doesn't exist.
                        - condition: ${e.code == 404}
                          steps:
                            - createDataset:
                                call: googleapis.bigquery.v2.datasets.insert
                                args:
                                    projectId: ${projectId}
                                    body:
                                        datasetReference:
                                            datasetId: ${datasetId}
                                            projectId: ${projectId}
                                        access[].role: "roles/bigquery.dataViewer"
                                        access[].specialGroup: "projectReaders"
                                        location: ${region}
                        - condition: ${e.code != 404}
                          steps:
                            - raiseError:
                                raise: ${e}

# This enrolls the recommenders API to the project so BQ can transfer from it.
# For official documentation visit:
# https://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq#set_up_the_export_using_bigquery_command_line_rest_api
enrollDataSourceIfNeeded:
  params: [projectId]
  steps:
  - checkIfDatasourceExists:
      try:
        call: googleapis.bigquerydatatransfer.v1.projects.dataSources.get
        args:
          name: ${"projects/" + projectId + "/dataSources/6063d10f-0000-2c12-a706-f403045e6250"}
        result: getResults
      except:
        as: e
        steps:
          - known_errors:
              switch:
                # Add the Datasource if it doesn't exist.
                  - condition: ${e.code == 400}
                    steps:
                      # there is a solid chance this will fail because the connector doesn't have this method listed
                      # https://cloud.google.com/workflows/docs/reference/googleapis/bigquerydatatransfer/v1/projects.locations.dataSources/get
                      - enrollInDatasource:
                          call: http.post
                          args:
                            url: ${"https://bigquerydatatransfer.googleapis.com/v1/projects/"+ projectId + ":enrollDataSources"}
                            body:
                              dataSourceIds:
                                - "6063d10f-0000-2c12-a706-f403045e6250"
                            auth:
                              type: OAuth2
                              scopes: "https://www.googleapis.com/auth/cloud-platform"
                  - condition: ${e.code != 400}
                    steps:
                      - raiseError:
                          raise: ${e}

# TODO(ghaun): Update so it doesn't create transfer configs on schedule. Currently makes it run ever 24 hours.
createDataExportIfNeeded:
  params: [projectId, orgId, datasetId]
  steps:
  - init:
      assign:
        - transferConfigId: null
  - checkIfTransferConfigExists:
      try:
        steps:
          - CheckInAllConfigs:
              call: paginateConfigs
              args:
                parent: ${"projects/" + projectId}
                transferConfigId: ${transferConfigId}
                pageToken: null
              result: transferConfigId
      except:
        as: e
        steps:
          # Add the transfer config
          - createTransferConfig:
              call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.create
              args:
                parent: ${"projects/" + projectId}
                body:
                  "name": "RecommenderAPIExport"
                  "displayName": "RecommenderAPIExport"
                  "dataSourceId": "6063d10f-0000-2c12-a706-f403045e6250"
                  "params":
                    "organization_id": ${orgId}
                  "destinationDatasetId": ${datasetId}
              result: createResult
          - finalCreated:
              assign:
                - transferConfigId: ${createResult.name}
  - finalReturn:
      return: ${transferConfigId}

# Recursively called step for handling pagination
paginateConfigs:
    params: [parent, transferConfigId, pageToken]
    steps:
      - listConfigsPage:
          call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.list
          args:
            parent: ${parent}
            pageToken: ${pageToken}
          result: pageResults
      - checkPageToken:
          try:
            steps:
              - setNextPageToken:
                  assign:
                    - nextPageToken: ${pageResults.nextPageToken}
          except:
            steps:
              - setNextPageTokenNull:
                  assign:
                    - nextPageToken: null
      - CheckResults:
          steps:
            - looping:
                for:
                  value: transferConfig
                  in: ${pageResults.transferConfigs}
                  steps:
                    - checkCondition:
                        switch:
                          - condition: ${transferConfig.displayName == "RecommenderAPIExport"}
                            steps:
                              - export_configId:
                                  assign:
                                    - transferConfigId: ${transferConfig.name}
                                  # returning as there is no need to continue looping.
                                  next: break
      - checkNextPage:
          switch:
            - condition: ${nextPageToken != null}
              steps:
                - recursiveCall:
                    call: paginateConfigs
                    args:
                      parent: ${parent}
                      transferConfigId: ${transferConfigId}
                      pageToken: ${nextPageToken}
                    result: transferConfigId
            - condition: ${transferConfigId == null}
              steps:
                - raiseMissingError:
                    raise: "RecommenderAPIExport was not found in transfer configs"
            - condition: True
              steps:
                - finalReturn:
                    return: ${transferConfigId}


runExportsInParallel:
  params: [bqDataset, bqTable, projectId, orgId, levelToExport, transferId]
  steps:
    - runningExports:
        parallel:
          shared: [bqDataset, bqTable, projectId]
          branches:
            # To make this call you need this API enabled: https://console.cloud.google.com/marketplace/product/google/cloudasset.googleapis.com?q=search&referrer=search&project=automating-container-rebuild
            # Service account needs cloud asset viewer. Probably best if you create a single service account for this.
            - assetInventoryBranch:
                steps:
                  - exportAssetInventory:
                    # Need to see if there is a workflows connector for this instead.
                      call: http.post
                      args:
                        url: ${"https://cloudasset.googleapis.com/v1/"+ levelToExport +":exportAssets"}
                        body:
                          outputConfig:
                            bigqueryDestination:
                              dataset: ${"projects/" + projectId + "/datasets/" + bqDataset}
                              table: ${bqTable}
                              # This will overwrite a table if data exists.
                              force: true
                              #partictionSpec: object: https://cloud.google.com/asset-inventory/docs/reference/rest/v1/TopLevel/exportAssets#partitionspec
                              #separateTablesPerAssetType:true  https://cloud.google.com/asset-inventory/docs/reference/rest/v1/TopLevel/exportAssets#BigQueryDestination
                        auth:
                          type: OAuth2
                          scopes: "https://www.googleapis.com/auth/cloud-platform"
                      result: response
            - recommenderApiBranch:
                steps:
                  # Parallel step requires more than one to run, and I want to leave it as a placeholder.
                  # Since manual run doesn't work as anticipated, this is just the get the list of runs.
                  # Simply Filler.
                  - listRuns:
                      call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.runs.list
                      args:
                        parent: ${transferId}
                  #- exportRecomendations:
                    #Note Org and Folder level exports is not GA and is in private preview. For now will focus on ORG
                    # One interesting thing to note is you have to use a BQ transfer service. This is gonna get interesting.
                    # One down side to this method is the fact that you can only export at the Org level, which is gonna cause me a few issues.
                    #  call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.startManualRuns
                    #  args:
                    #    parent: ${transferId}
                    #    body:
                    #      requested_run_time: ${time.format(sys.now())}
